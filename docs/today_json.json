{"SLAM": {"2401.03604": {"paper_title": "Amirkabir campus dataset: Real-world challenges and scenarios of Visual Inertial Odometry (VIO) for visually impaired people", "paper_abstract": "Visual Inertial Odometry (VIO) algorithms estimate the accurate camera trajectory by using camera and Inertial Measurement Unit (IMU) sensors. The applications of VIO span a diverse range, including augmented reality and indoor navigation. VIO algorithms hold the potential to facilitate navigation for visually impaired individuals in both indoor and outdoor settings. Nevertheless, state-of-the-art VIO algorithms encounter substantial challenges in dynamic environments, particularly in densely populated corridors. Existing VIO datasets, e.g., ADVIO, typically fail to effectively exploit these challenges. In this paper, we introduce the Amirkabir campus dataset (AUT-VI) to address the mentioned problem and improve the navigation systems. AUT-VI is a novel and super-challenging dataset with 126 diverse sequences in 17 different locations. This dataset contains dynamic objects, challenging loop-closure/map-reuse, different lighting conditions, reflections, and sudden camera movements to cover all extreme navigation scenarios. Moreover, in support of ongoing development efforts, we have released the Android application for data capture to the public. This allows fellow researchers to easily capture their customized VIO dataset variations. In addition, we evaluate state-of-the-art Visual Inertial Odometry (VIO) and Visual Odometry (VO) methods on our dataset, emphasizing the essential need for this challenging dataset.", "paper_authors": "Ali Samadzadeh, Mohammad Hassan Mojab, Heydar Soudani, Seyed Hesamoddin Mireshghollah, Ahmad Nickabadi", "update_time": "2024-01-07", "comments": "7 pages, 4 figures", "paper_url": "http://arxiv.org/abs/2401.03604", "paper_id": "2401.03604", "code_url": null}}, "SFM": {"2401.03450": {"paper_title": "A Classification of Critical Configurations for any Number of Projective Views", "paper_abstract": "Structure from motion is the process of recovering information about cameras and 3D scene from a set of images. Generally, in a noise-free setting, all information can be uniquely recovered if enough images and image points are provided. There are, however, certain cases where unique recovery is impossible, even in theory; these are called critical configurations. We use a recently developed algebraic approach to classify all critical configurations for any number of projective cameras. We show that they form well-known algebraic varieties, such as quadric surfaces and curves of degree at most 4. This paper also improves upon earlier results both by finding previously unknown critical configurations and by showing that some configurations previously believed to be critical are in fact not.", "paper_authors": "Martin Br\u00e5telund", "update_time": "2024-01-07", "comments": "44 pages, 10 figures, submitted to International Journal of Computer\n  Vision. arXiv admin note: text overlap with arXiv:2112.05478,\n  arXiv:2112.05074", "paper_url": "http://arxiv.org/abs/2401.03450", "paper_id": "2401.03450", "code_url": null}}, "Keypoint Detection": {"2401.03742": {"paper_title": "Flowmind2Digital: The First Comprehensive Flowmind Recognition and Conversion Approach", "paper_abstract": "Flowcharts and mind maps, collectively known as flowmind, are vital in daily activities, with hand-drawn versions facilitating real-time collaboration. However, there's a growing need to digitize them for efficient processing. Automated conversion methods are essential to overcome manual conversion challenges. Existing sketch recognition methods face limitations in practical situations, being field-specific and lacking digital conversion steps. Our paper introduces the Flowmind2digital method and hdFlowmind dataset to address these challenges. Flowmind2digital, utilizing neural networks and keypoint detection, achieves a record 87.3% accuracy on our dataset, surpassing previous methods by 11.9%. The hdFlowmind dataset, comprising 1,776 annotated flowminds across 22 scenarios, outperforms existing datasets. Additionally, our experiments emphasize the importance of simple graphics, enhancing accuracy by 9.3%.", "paper_authors": "Huanyu Liu, Jianfeng Cai, Tingjia Zhang, Hongsheng Li, Siyuan Wang, Guangming Zhu, Syed Afaq Ali Shah, Mohammed Bennamoun, Liang Zhang", "update_time": "2024-01-08", "comments": null, "paper_url": "http://arxiv.org/abs/2401.03742", "paper_id": "2401.03742", "code_url": null}}, "NeRF": {"2401.03890": {"paper_title": "A Survey on 3D Gaussian Splatting", "paper_abstract": "3D Gaussian splatting (3D GS) has recently emerged as a transformative technique in the explicit radiance field and computer graphics landscape. This innovative approach, characterized by the utilization of millions of 3D Gaussians, represents a significant departure from the neural radiance field (NeRF) methodologies, which predominantly use implicit, coordinate-based models to map spatial coordinates to pixel values. 3D GS, with its explicit scene representations and differentiable rendering algorithms, not only promises real-time rendering capabilities but also introduces unprecedented levels of control and editability. This positions 3D GS as a potential game-changer for the next generation of 3D reconstruction and representation. In the present paper, we provide the first systematic overview of the recent developments and critical contributions in the domain of 3D GS. We begin with a detailed exploration of the underlying principles and the driving forces behind the advent of 3D GS, setting the stage for understanding its significance. A focal point of our discussion is the practical applicability of 3D GS. By facilitating real-time performance, 3D GS opens up a plethora of applications, ranging from virtual reality to interactive media and beyond. This is complemented by a comparative analysis of leading 3D GS models, evaluated across various benchmark tasks to highlight their performance and practical utility. The survey concludes by identifying current challenges and suggesting potential avenues for future research in this domain. Through this survey, we aim to provide a valuable resource for both newcomers and seasoned researchers, fostering further exploration and advancement in applicable and explicit radiance field representation.", "paper_authors": "Guikun Chen, Wenguan Wang", "update_time": "2024-01-08", "comments": "Ongoing project", "paper_url": "http://arxiv.org/abs/2401.03890", "paper_id": "2401.03890", "code_url": null}, "2401.03771": {"paper_title": "NeRFmentation: NeRF-based Augmentation for Monocular Depth Estimation", "paper_abstract": "The capabilities of monocular depth estimation (MDE) models are limited by the availability of sufficient and diverse datasets. In the case of MDE models for autonomous driving, this issue is exacerbated by the linearity of the captured data trajectories. We propose a NeRF-based data augmentation pipeline to introduce synthetic data with more diverse viewing directions into training datasets and demonstrate the benefits of our approach to model performance and robustness. Our data augmentation pipeline, which we call \"NeRFmentation\", trains NeRFs on each scene in the dataset, filters out subpar NeRFs based on relevant metrics, and uses them to generate synthetic RGB-D images captured from new viewing directions. In this work, we apply our technique in conjunction with three state-of-the-art MDE architectures on the popular autonomous driving dataset KITTI, augmenting its training set of the Eigen split. We evaluate the resulting performance gain on the original test set, a separate popular driving set, and our own synthetic test set.", "paper_authors": "Casimir Feldmann, Niall Siegenheim, Nikolas Hars, Lovro Rabuzin, Mert Ertugrul, Luca Wolfart, Marc Pollefeys, Zuria Bauer, Martin R. Oswald", "update_time": "2024-01-08", "comments": null, "paper_url": "http://arxiv.org/abs/2401.03771", "paper_id": "2401.03771", "code_url": null}, "2401.03257": {"paper_title": "RustNeRF: Robust Neural Radiance Field with Low-Quality Images", "paper_abstract": "Recent work on Neural Radiance Fields (NeRF) exploits multi-view 3D consistency, achieving impressive results in 3D scene modeling and high-fidelity novel-view synthesis. However, there are limitations. First, existing methods assume enough high-quality images are available for training the NeRF model, ignoring real-world image degradation. Second, previous methods struggle with ambiguity in the training set due to unmodeled inconsistencies among different views. In this work, we present RustNeRF for real-world high-quality NeRF. To improve NeRF's robustness under real-world inputs, we train a 3D-aware preprocessing network that incorporates real-world degradation modeling. We propose a novel implicit multi-view guidance to address information loss during image degradation and restoration. Extensive experiments demonstrate RustNeRF's advantages over existing approaches under real-world degradation. The code will be released.", "paper_authors": "Mengfei Li, Ming Lu, Xiaofang Li, Shanghang Zhang", "update_time": "2024-01-06", "comments": null, "paper_url": "http://arxiv.org/abs/2401.03257", "paper_id": "2401.03257", "code_url": null}, "2401.03203": {"paper_title": "Hi-Map: Hierarchical Factorized Radiance Field for High-Fidelity Monocular Dense Mapping", "paper_abstract": "In this paper, we introduce Hi-Map, a novel monocular dense mapping approach based on Neural Radiance Field (NeRF). Hi-Map is exceptional in its capacity to achieve efficient and high-fidelity mapping using only posed RGB inputs. Our method eliminates the need for external depth priors derived from e.g., a depth estimation model. Our key idea is to represent the scene as a hierarchical feature grid that encodes the radiance and then factorizes it into feature planes and vectors. As such, the scene representation becomes simpler and more generalizable for fast and smooth convergence on new observations. This allows for efficient computation while alleviating noise patterns by reducing the complexity of the scene representation. Buttressed by the hierarchical factorized representation, we leverage the Sign Distance Field (SDF) as a proxy of rendering for inferring the volume density, demonstrating high mapping fidelity. Moreover, we introduce a dual-path encoding strategy to strengthen the photometric cues and further boost the mapping quality, especially for the distant and textureless regions. Extensive experiments demonstrate our method's superiority in geometric and textural accuracy over the state-of-the-art NeRF-based monocular mapping methods.", "paper_authors": "Tongyan Hua, Haotian Bai, Zidong Cao, Ming Liu, Dacheng Tao, Lin Wang", "update_time": "2024-01-06", "comments": null, "paper_url": "http://arxiv.org/abs/2401.03203", "paper_id": "2401.03203", "code_url": null}}}