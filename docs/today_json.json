{"Visual Localization": {"2401.16459": {"paper_title": "Bridging Generative and Discriminative Models for Unified Visual Perception with Diffusion Priors", "paper_abstract": "The remarkable prowess of diffusion models in image generation has spurred efforts to extend their application beyond generative tasks. However, a persistent challenge exists in lacking a unified approach to apply diffusion models to visual perception tasks with diverse semantic granularity requirements. Our purpose is to establish a unified visual perception framework, capitalizing on the potential synergies between generative and discriminative models. In this paper, we propose Vermouth, a simple yet effective framework comprising a pre-trained Stable Diffusion (SD) model containing rich generative priors, a unified head (U-head) capable of integrating hierarchical representations, and an adapted expert providing discriminative priors. Comprehensive investigations unveil potential characteristics of Vermouth, such as varying granularity of perception concealed in latent variables at distinct time steps and various U-net stages. We emphasize that there is no necessity for incorporating a heavyweight or intricate decoder to transform diffusion models into potent representation learners. Extensive comparative evaluations against tailored discriminative models showcase the efficacy of our approach on zero-shot sketch-based image retrieval (ZS-SBIR), few-shot classification, and open-vocabulary semantic segmentation tasks. The promising results demonstrate the potential of diffusion models as formidable learners, establishing their significance in furnishing informative and robust visual representations.", "paper_authors": "Shiyin Dong, Mingrui Zhu, Kun Cheng, Nannan Wang, Xinbo Gao", "update_time": "2024-01-29", "comments": "18 pages,11 figures", "paper_url": "http://arxiv.org/abs/2401.16459", "paper_id": "2401.16459", "code_url": null}}, "NeRF": {"2401.17121": {"paper_title": "Physical Priors Augmented Event-Based 3D Reconstruction", "paper_abstract": "3D neural implicit representations play a significant component in many robotic applications. However, reconstructing neural radiance fields (NeRF) from realistic event data remains a challenge due to the sparsities and the lack of information when only event streams are available. In this paper, we utilize motion, geometry, and density priors behind event data to impose strong physical constraints to augment NeRF training. The proposed novel pipeline can directly benefit from those priors to reconstruct 3D scenes without additional inputs. Moreover, we present a novel density-guided patch-based sampling strategy for robust and efficient learning, which not only accelerates training procedures but also conduces to expressions of local geometries. More importantly, we establish the first large dataset for event-based 3D reconstruction, which contains 101 objects with various materials and geometries, along with the groundtruth of images and depth maps for all camera viewpoints, which significantly facilitates other research in the related fields. The code and dataset will be publicly available at https://github.com/Mercerai/PAEv3d.", "paper_authors": "Jiaxu Wang, Junhao He, Ziyi Zhang, Renjing Xu", "update_time": "2024-01-30", "comments": "6 pages, 6 figures, ICRA 2024", "paper_url": "http://arxiv.org/abs/2401.17121", "paper_id": "2401.17121", "code_url": null}}}