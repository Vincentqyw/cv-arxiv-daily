{"NeRF": {"2402.01524": {"paper_title": "HyperPlanes: Hypernetwork Approach to Rapid NeRF Adaptation", "paper_abstract": "Neural radiance fields (NeRFs) are a widely accepted standard for synthesizing new 3D object views from a small number of base images. However, NeRFs have limited generalization properties, which means that we need to use significant computational resources to train individual architectures for each item we want to represent. To address this issue, we propose a few-shot learning approach based on the hypernetwork paradigm that does not require gradient optimization during inference. The hypernetwork gathers information from the training data and generates an update for universal weights. As a result, we have developed an efficient method for generating a high-quality 3D object representation from a small number of images in a single step. This has been confirmed by direct comparison with the state-of-the-art solutions and a comprehensive ablation study.", "paper_authors": "Pawe\u0142 Batorski, Dawid Malarz, Marcin Przewi\u0119\u017alikowski, Marcin Mazur, S\u0142awomir Tadeja, Przemys\u0142aw Spurek", "update_time": "2024-02-02", "comments": null, "paper_url": "http://arxiv.org/abs/2402.01524", "paper_id": "2402.01524", "code_url": "https://github.com/gmum/hyperplanes"}, "2402.01485": {"paper_title": "Di-NeRF: Distributed NeRF for Collaborative Learning with Unknown Relative Poses", "paper_abstract": "Collaborative mapping of unknown environments can be done faster and more robustly than a single robot. However, a collaborative approach requires a distributed paradigm to be scalable and deal with communication issues. This work presents a fully distributed algorithm enabling a group of robots to collectively optimize the parameters of a Neural Radiance Field (NeRF). The algorithm involves the communication of each robot's trained NeRF parameters over a mesh network, where each robot trains its NeRF and has access to its own visual data only. Additionally, the relative poses of all robots are jointly optimized alongside the model parameters, enabling mapping with unknown relative camera poses. We show that multi-robot systems can benefit from differentiable and robust 3D reconstruction optimized from multiple NeRFs. Experiments on real-world and synthetic data demonstrate the efficiency of the proposed algorithm. See the website of the project for videos of the experiments and supplementary material(https://sites.google.com/view/di-nerf/home).", "paper_authors": "Mahboubeh Asadi, Kourosh Zareinia, Sajad Saeedi", "update_time": "2024-02-02", "comments": "9 pages, 11 figures, Submitted to IEEE-RA-L", "paper_url": "http://arxiv.org/abs/2402.01485", "paper_id": "2402.01485", "code_url": null}, "2402.01459": {"paper_title": "GaMeS: Mesh-Based Adapting and Modification of Gaussian Splatting", "paper_abstract": "In recent years, a range of neural network-based methods for image rendering have been introduced. For instance, widely-researched neural radiance fields (NeRF) rely on a neural network to represent 3D scenes, allowing for realistic view synthesis from a small number of 2D images. However, most NeRF models are constrained by long training and inference times. In comparison, Gaussian Splatting (GS) is a novel, state-of-theart technique for rendering points in a 3D scene by approximating their contribution to image pixels through Gaussian distributions, warranting fast training and swift, real-time rendering. A drawback of GS is the absence of a well-defined approach for its conditioning due to the necessity to condition several hundred thousand Gaussian components. To solve this, we introduce Gaussian Mesh Splatting (GaMeS) model, a hybrid of mesh and a Gaussian distribution, that pin all Gaussians splats on the object surface (mesh). The unique contribution of our methods is defining Gaussian splats solely based on their location on the mesh, allowing for automatic adjustments in position, scale, and rotation during animation. As a result, we obtain high-quality renders in the real-time generation of high-quality views. Furthermore, we demonstrate that in the absence of a predefined mesh, it is possible to fine-tune the initial mesh during the learning process.", "paper_authors": "Joanna Waczy\u0144ska, Piotr Borycki, S\u0142awomir Tadeja, Jacek Tabor, Przemys\u0142aw Spurek", "update_time": "2024-02-02", "comments": null, "paper_url": "http://arxiv.org/abs/2402.01459", "paper_id": "2402.01459", "code_url": "https://github.com/waczjoan/gaussian-mesh-splatting"}, "2402.01380": {"paper_title": "Efficient Dynamic-NeRF Based Volumetric Video Coding with Rate Distortion Optimization", "paper_abstract": "Volumetric videos, benefiting from immersive 3D realism and interactivity, hold vast potential for various applications, while the tremendous data volume poses significant challenges for compression. Recently, NeRF has demonstrated remarkable potential in volumetric video compression thanks to its simple representation and powerful 3D modeling capabilities, where a notable work is ReRF. However, ReRF separates the modeling from compression process, resulting in suboptimal compression efficiency. In contrast, in this paper, we propose a volumetric video compression method based on dynamic NeRF in a more compact manner. Specifically, we decompose the NeRF representation into the coefficient fields and the basis fields, incrementally updating the basis fields in the temporal domain to achieve dynamic modeling. Additionally, we perform end-to-end joint optimization on the modeling and compression process to further improve the compression efficiency. Extensive experiments demonstrate that our method achieves higher compression efficiency compared to ReRF on various datasets.", "paper_authors": "Zhiyu Zhang, Guo Lu, Huanxiong Liang, Anni Tang, Qiang Hu, Li Song", "update_time": "2024-02-02", "comments": null, "paper_url": "http://arxiv.org/abs/2402.01380", "paper_id": "2402.01380", "code_url": null}, "2402.01217": {"paper_title": "Taming Uncertainty in Sparse-view Generalizable NeRF via Indirect Diffusion Guidance", "paper_abstract": "Neural Radiance Fields (NeRF) have demonstrated effectiveness in synthesizing novel views. However, their reliance on dense inputs and scene-specific optimization has limited their broader applicability. Generalizable NeRFs (Gen-NeRF), while intended to address this, often produce blurring artifacts in unobserved regions with sparse inputs, which are full of uncertainty. In this paper, we aim to diminish the uncertainty in Gen-NeRF for plausible renderings. We assume that NeRF's inability to effectively mitigate this uncertainty stems from its inherent lack of generative capacity. Therefore, we innovatively propose an Indirect Diffusion-guided NeRF framework, termed ID-NeRF, to address this uncertainty from a generative perspective by leveraging a distilled diffusion prior as guidance. Specifically, to avoid model confusion caused by directly regularizing with inconsistent samplings as in previous methods, our approach introduces a strategy to indirectly inject the inherently missing imagination into the learned implicit function through a diffusion-guided latent space. Empirical evaluation across various benchmarks demonstrates the superior performance of our approach in handling uncertainty with sparse inputs.", "paper_authors": "Yaokun Li, Chao Gou, Guang Tan", "update_time": "2024-02-02", "comments": null, "paper_url": "http://arxiv.org/abs/2402.01217", "paper_id": "2402.01217", "code_url": null}}}