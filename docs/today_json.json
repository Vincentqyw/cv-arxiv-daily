{"SLAM": {"2312.05889": {"paper_title": "SuperPrimitive: Scene Reconstruction at a Primitive Level", "paper_abstract": "Joint camera pose and dense geometry estimation from a set of images or a monocular video remains a challenging problem due to its computational complexity and inherent visual ambiguities. Most dense incremental reconstruction systems operate directly on image pixels and solve for their 3D positions using multi-view geometry cues. Such pixel-level approaches suffer from ambiguities or violations of multi-view consistency (e.g. caused by textureless or specular surfaces).   We address this issue with a new image representation which we call a SuperPrimitive. SuperPrimitives are obtained by splitting images into semantically correlated local regions and enhancing them with estimated surface normal directions, both of which are predicted by state-of-the-art single image neural networks. This provides a local geometry estimate per SuperPrimitive, while their relative positions are adjusted based on multi-view observations.   We demonstrate the versatility of our new representation by addressing three 3D reconstruction tasks: depth completion, few-view structure from motion, and monocular dense visual odometry.", "paper_authors": "Kirill Mazur, Gwangbin Bae, Andrew J. Davison", "update_time": "2023-12-10", "comments": null, "paper_url": "http://arxiv.org/abs/2312.05889", "paper_id": "2312.05889", "code_url": null}}, "SFM": {"2312.05889": {"paper_title": "SuperPrimitive: Scene Reconstruction at a Primitive Level", "paper_abstract": "Joint camera pose and dense geometry estimation from a set of images or a monocular video remains a challenging problem due to its computational complexity and inherent visual ambiguities. Most dense incremental reconstruction systems operate directly on image pixels and solve for their 3D positions using multi-view geometry cues. Such pixel-level approaches suffer from ambiguities or violations of multi-view consistency (e.g. caused by textureless or specular surfaces).   We address this issue with a new image representation which we call a SuperPrimitive. SuperPrimitives are obtained by splitting images into semantically correlated local regions and enhancing them with estimated surface normal directions, both of which are predicted by state-of-the-art single image neural networks. This provides a local geometry estimate per SuperPrimitive, while their relative positions are adjusted based on multi-view observations.   We demonstrate the versatility of our new representation by addressing three 3D reconstruction tasks: depth completion, few-view structure from motion, and monocular dense visual odometry.", "paper_authors": "Kirill Mazur, Gwangbin Bae, Andrew J. Davison", "update_time": "2023-12-10", "comments": null, "paper_url": "http://arxiv.org/abs/2312.05889", "paper_id": "2312.05889", "code_url": null}}, "Visual Localization": {"2312.06179": {"paper_title": "Dynamic Weighted Combiner for Mixed-Modal Image Retrieval", "paper_abstract": "Mixed-Modal Image Retrieval (MMIR) as a flexible search paradigm has attracted wide attention. However, previous approaches always achieve limited performance, due to two critical factors are seriously overlooked. 1) The contribution of image and text modalities is different, but incorrectly treated equally. 2) There exist inherent labeling noises in describing users' intentions with text in web datasets from diverse real-world scenarios, giving rise to overfitting. We propose a Dynamic Weighted Combiner (DWC) to tackle the above challenges, which includes three merits. First, we propose an Editable Modality De-equalizer (EMD) by taking into account the contribution disparity between modalities, containing two modality feature editors and an adaptive weighted combiner. Second, to alleviate labeling noises and data bias, we propose a dynamic soft-similarity label generator (SSG) to implicitly improve noisy supervision. Finally, to bridge modality gaps and facilitate similarity learning, we propose a CLIP-based mutual enhancement module alternately trained by a mixed-modality contrastive loss. Extensive experiments verify that our proposed model significantly outperforms state-of-the-art methods on real-world datasets. The source code is available at \\url{https://github.com/fuxianghuang1/DWC}.", "paper_authors": "Fuxiang Huang, Lei Zhang, Xiaowei Fu, Suqi Song", "update_time": "2023-12-11", "comments": "11 pages, 12 figures and 12 tables. To appear in AAAI 2024", "paper_url": "http://arxiv.org/abs/2312.06179", "paper_id": "2312.06179", "code_url": null}}, "NeRF": {"2312.06642": {"paper_title": "CorresNeRF: Image Correspondence Priors for Neural Radiance Fields", "paper_abstract": "Neural Radiance Fields (NeRFs) have achieved impressive results in novel view synthesis and surface reconstruction tasks. However, their performance suffers under challenging scenarios with sparse input views. We present CorresNeRF, a novel method that leverages image correspondence priors computed by off-the-shelf methods to supervise NeRF training. We design adaptive processes for augmentation and filtering to generate dense and high-quality correspondences. The correspondences are then used to regularize NeRF training via the correspondence pixel reprojection and depth loss terms. We evaluate our methods on novel view synthesis and surface reconstruction tasks with density-based and SDF-based NeRF models on different datasets. Our method outperforms previous methods in both photometric and geometric metrics. We show that this simple yet effective technique of using correspondence priors can be applied as a plug-and-play module across different NeRF variants. The project page is at https://yxlao.github.io/corres-nerf.", "paper_authors": "Yixing Lao, Xiaogang Xu, Zhipeng Cai, Xihui Liu, Hengshuang Zhao", "update_time": "2023-12-11", "comments": null, "paper_url": "http://arxiv.org/abs/2312.06642", "paper_id": "2312.06642", "code_url": "https://github.com/yxlao/corres-nerf"}, "2312.06439": {"paper_title": "DreamControl: Control-Based Text-to-3D Generation with 3D Self-Prior", "paper_abstract": "3D generation has raised great attention in recent years. With the success of text-to-image diffusion models, the 2D-lifting technique becomes a promising route to controllable 3D generation. However, these methods tend to present inconsistent geometry, which is also known as the Janus problem. We observe that the problem is caused mainly by two aspects, i.e., viewpoint bias in 2D diffusion models and overfitting of the optimization objective. To address it, we propose a two-stage 2D-lifting framework, namely DreamControl, which optimizes coarse NeRF scenes as 3D self-prior and then generates fine-grained objects with control-based score distillation. Specifically, adaptive viewpoint sampling and boundary integrity metric are proposed to ensure the consistency of generated priors. The priors are then regarded as input conditions to maintain reasonable geometries, in which conditional LoRA and weighted score are further proposed to optimize detailed textures. DreamControl can generate high-quality 3D content in terms of both geometry consistency and texture fidelity. Moreover, our control-based optimization guidance is applicable to more downstream tasks, including user-guided generation and 3D animation. The project page is available at https://github.com/tyhuang0428/DreamControl.", "paper_authors": "Tianyu Huang, Yihan Zeng, Zhilu Zhang, Wan Xu, Hang Xu, Songcen Xu, Rynson W. H. Lau, Wangmeng Zuo", "update_time": "2023-12-11", "comments": null, "paper_url": "http://arxiv.org/abs/2312.06439", "paper_id": "2312.06439", "code_url": null}, "2312.05855": {"paper_title": "NeVRF: Neural Video-based Radiance Fields for Long-duration Sequences", "paper_abstract": "Adopting Neural Radiance Fields (NeRF) to long-duration dynamic sequences has been challenging. Existing methods struggle to balance between quality and storage size and encounter difficulties with complex scene changes such as topological changes and large motions. To tackle these issues, we propose a novel neural video-based radiance fields (NeVRF) representation. NeVRF marries neural radiance field with image-based rendering to support photo-realistic novel view synthesis on long-duration dynamic inward-looking scenes. We introduce a novel multi-view radiance blending approach to predict radiance directly from multi-view videos. By incorporating continual learning techniques, NeVRF can efficiently reconstruct frames from sequential data without revisiting previous frames, enabling long-duration free-viewpoint video. Furthermore, with a tailored compression approach, NeVRF can compactly represent dynamic scenes, making dynamic radiance fields more practical in real-world scenarios. Our extensive experiments demonstrate the effectiveness of NeVRF in enabling long-duration sequence rendering, sequential data reconstruction, and compact data storage.", "paper_authors": "Minye Wu, Tinne Tuytelaars", "update_time": "2023-12-10", "comments": "11 pages, 12 figures", "paper_url": "http://arxiv.org/abs/2312.05855", "paper_id": "2312.05855", "code_url": null}, "2312.05748": {"paper_title": "IL-NeRF: Incremental Learning for Neural Radiance Fields with Camera Pose Alignment", "paper_abstract": "Neural radiance fields (NeRF) is a promising approach for generating photorealistic images and representing complex scenes. However, when processing data sequentially, it can suffer from catastrophic forgetting, where previous data is easily forgotten after training with new data. Existing incremental learning methods using knowledge distillation assume that continuous data chunks contain both 2D images and corresponding camera pose parameters, pre-estimated from the complete dataset. This poses a paradox as the necessary camera pose must be estimated from the entire dataset, even though the data arrives sequentially and future chunks are inaccessible. In contrast, we focus on a practical scenario where camera poses are unknown. We propose IL-NeRF, a novel framework for incremental NeRF training, to address this challenge. IL-NeRF's key idea lies in selecting a set of past camera poses as references to initialize and align the camera poses of incoming image data. This is followed by a joint optimization of camera poses and replay-based NeRF distillation. Our experiments on real-world indoor and outdoor scenes show that IL-NeRF handles incremental NeRF training and outperforms the baselines by up to $54.04\\%$ in rendering quality.", "paper_authors": "Letian Zhang, Ming Li, Chen Chen, Jie Xu", "update_time": "2023-12-10", "comments": null, "paper_url": "http://arxiv.org/abs/2312.05748", "paper_id": "2312.05748", "code_url": null}, "2312.05664": {"paper_title": "CoGS: Controllable Gaussian Splatting", "paper_abstract": "Capturing and re-animating the 3D structure of articulated objects present significant barriers. On one hand, methods requiring extensively calibrated multi-view setups are prohibitively complex and resource-intensive, limiting their practical applicability. On the other hand, while single-camera Neural Radiance Fields (NeRFs) offer a more streamlined approach, they have excessive training and rendering costs. 3D Gaussian Splatting would be a suitable alternative but for two reasons. Firstly, existing methods for 3D dynamic Gaussians require synchronized multi-view cameras, and secondly, the lack of controllability in dynamic scenarios. We present CoGS, a method for Controllable Gaussian Splatting, that enables the direct manipulation of scene elements, offering real-time control of dynamic scenes without the prerequisite of pre-computing control signals. We evaluated CoGS using both synthetic and real-world datasets that include dynamic objects that differ in degree of difficulty. In our evaluations, CoGS consistently outperformed existing dynamic and controllable neural representations in terms of visual fidelity.", "paper_authors": "Heng Yu, Joel Julin, Zolt\u00e1n \u00c1. Milacski, Koichiro Niinuma, L\u00e1szl\u00f3 A. Jeni", "update_time": "2023-12-09", "comments": "10 pages, in submission", "paper_url": "http://arxiv.org/abs/2312.05664", "paper_id": "2312.05664", "code_url": null}, "2312.05572": {"paper_title": "R2-Talker: Realistic Real-Time Talking Head Synthesis with Hash Grid Landmarks Encoding and Progressive Multilayer Conditioning", "paper_abstract": "Dynamic NeRFs have recently garnered growing attention for 3D talking portrait synthesis. Despite advances in rendering speed and visual quality, challenges persist in enhancing efficiency and effectiveness. We present R2-Talker, an efficient and effective framework enabling realistic real-time talking head synthesis. Specifically, using multi-resolution hash grids, we introduce a novel approach for encoding facial landmarks as conditional features. This approach losslessly encodes landmark structures as conditional features, decoupling input diversity, and conditional spaces by mapping arbitrary landmarks to a unified feature space. We further propose a scheme of progressive multilayer conditioning in the NeRF rendering pipeline for effective conditional feature fusion. Our new approach has the following advantages as demonstrated by extensive experiments compared with the state-of-the-art works: 1) The lossless input encoding enables acquiring more precise features, yielding superior visual quality. The decoupling of inputs and conditional spaces improves generalizability. 2) The fusing of conditional features and MLP outputs at each MLP layer enhances conditional impact, resulting in more accurate lip synthesis and better visual quality. 3) It compactly structures the fusion of conditional features, significantly enhancing computational efficiency.", "paper_authors": "Zhiling Ye, LiangGuo Zhang, Dingheng Zeng, Quan Lu, Ning Jiang", "update_time": "2023-12-09", "comments": null, "paper_url": "http://arxiv.org/abs/2312.05572", "paper_id": "2312.05572", "code_url": null}, "2312.05330": {"paper_title": "Multi-view Inversion for 3D-aware Generative Adversarial Networks", "paper_abstract": "Current 3D GAN inversion methods for human heads typically use only one single frontal image to reconstruct the whole 3D head model. This leaves out meaningful information when multi-view data or dynamic videos are available. Our method builds on existing state-of-the-art 3D GAN inversion techniques to allow for consistent and simultaneous inversion of multiple views of the same subject. We employ a multi-latent extension to handle inconsistencies present in dynamic face videos to re-synthesize consistent 3D representations from the sequence. As our method uses additional information about the target subject, we observe significant enhancements in both geometric accuracy and image quality, particularly when rendering from wide viewing angles. Moreover, we demonstrate the editability of our inverted 3D renderings, which distinguishes them from NeRF-based scene reconstructions.", "paper_authors": "Florian Barthel, Anna Hilsmann, Peter Eisert", "update_time": "2023-12-08", "comments": null, "paper_url": "http://arxiv.org/abs/2312.05330", "paper_id": "2312.05330", "code_url": null}}}